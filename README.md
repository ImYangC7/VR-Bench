# VR-Bench: Visual Reasoning Benchmark for Vision-Language Models

[ä¸­æ–‡æ–‡æ¡£](README_CN.md) | English

VR-Bench is a comprehensive benchmark for evaluating Vision-Language Models (VLMs) on spatial reasoning and planning tasks through various puzzle games. It provides a unified framework for dataset generation, evaluation, and analysis.

## ğŸ® Supported Games

VR-Bench includes five different puzzle games, each testing different aspects of visual reasoning:

- **Maze**: Navigate from start to goal in a grid-based maze
- **Sokoban**: Push boxes to target positions while avoiding walls
- **3D Maze**: Multi-level maze with ladders connecting different floors
- **PathFinder**: Find paths through irregular mazes with labeled waypoints
- **TrapField**: Navigate through a field while avoiding traps

## âœ¨ Key Features

- **Procedural Generation**: Automatically generate diverse puzzle levels with configurable difficulty
- **Texture Customization**: Support for custom visual themes through texture skins
- **Video Rendering**: Generate solution videos with smooth animations (24 FPS)
- **VLM Evaluation**: Built-in framework for testing various VLMs (GPT, Gemini, Qwen, etc.)
- **Comprehensive Metrics**: Success Rate (SR), Path Ratio (PR), Move Ratio (MR)
- **Parallel Processing**: Multi-threaded generation and evaluation for efficiency
- **Deduplication**: Automatic detection and removal of duplicate levels

## ğŸ“‹ Requirements

- Python >= 3.10
- CUDA-compatible GPU (optional, for local VLM inference)

## ğŸš€ Quick Start

### 1. Installation

```bash
# Clone the repository
git clone https://github.com/SNHuan/VR-Bench.git
cd VR-Bench

# Install dependencies
pip install -r requirements.txt
```

### 2. Environment Configuration

```bash
# Copy environment template
cp .env.example .env

# Edit .env with your settings
# - API keys for VLM evaluation
# - Dataset paths
# - CUDA configuration
```

### 3. Download Dataset

```bash
# Download pre-generated dataset from Hugging Face
python dataset_init.py --output-dir ./dataset_VR
```

### 4. Generate Custom Levels

```bash
# Edit config/config.yaml to configure game type and difficulty
# Then run batch generation
python -m generation.batch_generate config/config.yaml
```

### 5. Evaluate VLMs

```bash
# Start local VLM server (optional, for local models)
bash scripts/start_sglang_server.sh

# Run evaluation
bash scripts/run_vlm_eval.sh
```

## ğŸ“ Project Structure

```
VR-Bench/
â”œâ”€â”€ core/                   # Core framework
â”‚   â”œâ”€â”€ schema/            # Unified state representation
â”‚   â”œâ”€â”€ renderer.py        # Base rendering engine
â”‚   â”œâ”€â”€ texture_handler.py # Texture management
â”‚   â””â”€â”€ game_adapter.py    # Game adapter interface
â”œâ”€â”€ games/                 # Game implementations
â”‚   â”œâ”€â”€ maze/             # Maze game
â”‚   â”œâ”€â”€ sokoban/          # Sokoban game
â”‚   â”œâ”€â”€ maze3d/           # 3D Maze game
â”‚   â”œâ”€â”€ pathfinder/       # PathFinder game
â”‚   â””â”€â”€ trapfield/        # TrapField game
â”œâ”€â”€ generation/           # Dataset generation
â”‚   â”œâ”€â”€ batch_generate.py # Batch generation tool
â”‚   â””â”€â”€ generate_videos.py # Video generation
â”œâ”€â”€ evaluation/           # VLM evaluation
â”‚   â””â”€â”€ vlm_eval/        # Evaluation framework
â”œâ”€â”€ config/              # Configuration files
â”‚   â”œâ”€â”€ config.yaml      # Generation config
â”‚   â””â”€â”€ vlm/            # Evaluation configs
â”œâ”€â”€ skins/              # Texture assets
â””â”€â”€ scripts/            # Utility scripts
```

## ğŸ¯ Usage Examples

### Generate Maze Dataset

```bash
# Edit config/config.yaml
game_type: "maze"
skins_root: "skins/maze"
difficulties:
  small:
    maze_size: 9
    count: 100

# Run generation
python -m generation.batch_generate config/config.yaml
```

### Evaluate on Sokoban

```bash
# Edit config/vlm/sokoban_eval.yaml
# Configure models and dataset path

# Run evaluation
python -m evaluation.vlm_eval.run_vlm_eval config/vlm/sokoban_eval.yaml
```

## ğŸ“Š Evaluation Metrics

- **Success Rate (SR)**: Percentage of levels solved correctly
- **Path Ratio (PR)**: Ratio of correct consecutive actions from the start
- **Move Ratio (MR)**: Binary metric for exact solution match
- **Step Count**: Number of actions in the solution

## ğŸ”§ Configuration

### Generation Config (`config/config.yaml`)

- `game_type`: Game to generate (maze, sokoban, pathfinder, trapfield, maze3d)
- `skins_root`: Path to texture assets
- `difficulties`: Difficulty levels and parameters
- `generation.max_attempts`: Max attempts to generate valid level
- `parallel.max_workers`: Number of parallel workers

### Evaluation Config (`config/vlm/*.yaml`)

- `game`: Game type to evaluate
- `dataset`: Path to dataset
- `models`: List of VLMs to test
- `workers`: Number of parallel evaluation workers
- `max_levels`: Maximum levels to evaluate (-1 for all)

## ğŸ¨ Custom Textures

Each game supports custom texture skins for visual variety:

1. Create a new folder under `skins/<game_name>/`
2. Add required texture images (PNG/JPG format)
3. Specify the skin path in configuration

Required texture files vary by game. Refer to existing skin folders for examples.

### Texture Requirements by Game

- **Maze**: wall, floor, player, goal
- **Sokoban**: wall, floor, player, box, target
- **PathFinder**: Custom background and path textures
- **TrapField**: floor, trap, player, goal

## ğŸ”¬ Adding New Games

VR-Bench uses an adapter pattern for easy extensibility:

1. Create a new game directory under `games/`
2. Implement the `GameAdapter` interface:
   - `generate_level()`: Level generation logic
   - `save_level()`: Save level data and render outputs
   - `get_level_hash()`: For deduplication
   - `is_duplicate()`: Duplicate detection
3. Implement game-specific logic and rendering
4. Create an executor in `evaluation/vlm_eval/executors/`
5. Register in `generation/batch_generate.py`

See existing game implementations for reference.

## ğŸ› Troubleshooting

### Common Issues

**Issue**: CUDA out of memory during VLM inference
- **Solution**: Reduce batch size or use tensor parallelism with multiple GPUs

**Issue**: Video generation fails
- **Solution**: Ensure ffmpeg is installed: `pip install imageio-ffmpeg`

**Issue**: API rate limiting
- **Solution**: Reduce `workers` in evaluation config or add delays

**Issue**: Duplicate levels generated
- **Solution**: Increase `max_duplicate_retries` in generation config

## ğŸ“š Citation

If you use VR-Bench in your research, please cite:

```bibtex
@misc{vrbench2025,
  title={VR-Bench: Visual Reasoning Benchmark for Vision-Language Models},
  author={VR-Bench Team},
  year={2025},
  url={https://github.com/SNHuan/VR-Bench}
}
```

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request. For major changes:

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

## ğŸ”— Related Resources

- [Hugging Face Dataset](https://huggingface.co/datasets/amagipeng/VR-Bench)

## ğŸ“ License

This project is licensed under the MIT License - see the LICENSE file for details.

## ğŸ™ Acknowledgments

VR-Bench builds upon various open-source projects and research in visual reasoning and VLM evaluation.

## ğŸ“§ Contact

For questions and feedback, please open an issue on GitHub or contact the maintainers.

